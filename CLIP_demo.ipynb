{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/cedro3/others/blob/master/CLIP_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXnkFjoZy9kd"
   },
   "source": [
    "# セットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0BpdJkdBssk9"
   },
   "outputs": [],
   "source": [
    "# --- セットアップ ---\n",
    "\n",
    "# 1.pytorchバージョン変更\n",
    "! pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 -f https://download.pytorch.org/whl/torch_stable.html #ftfy regex\n",
    "\n",
    "# 2.GithubからCLIPをコピー\n",
    "! git clone https://github.com/openai/CLIP.git\n",
    "%cd CLIP/clip\n",
    "\n",
    "# 3.CLIPモデルの重みをダウンロード\n",
    "MODELS = {\n",
    "    \"RN50\": \"https://openaipublic.azureedge.net/clip/models/afeb0e10f9e5a86da6080e35cf09123aca3b358a0c3e3b6c78a7b63bc04b6762/RN50.pt\",\n",
    "    \"RN101\": \"https://openaipublic.azureedge.net/clip/models/8fa8567bab74a42d41c5915025a8e4538c3bdbe8804a470a72f30b0d94fab599/RN101.pt\",\n",
    "    \"RN50x4\": \"https://openaipublic.azureedge.net/clip/models/7e526bd135e493cef0776de27d5f42653e6b4c8bf9e0f653bb11773263205fdd/RN50x4.pt\",\n",
    "    \"ViT-B/32\": \"https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\",    \n",
    "}\n",
    "! wget {MODELS[\"ViT-B/32\"]} -O model.pt\n",
    "\n",
    "# 4.simple_tokenizer インストール\n",
    "! pip install ftfy regex\n",
    "from simple_tokenizer import *\n",
    "tokenizer = SimpleTokenizer()\n",
    "\n",
    "# 5.サンプル画像ダウンロード\n",
    "import gdown\n",
    "gdown.download('https://drive.google.com/uc?id=1vcxH6JOtwh_-FoZ8SNXYlHF9qCi3YoDH', 'food_101.zip', quiet=False)\n",
    "! unzip food_101.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJCVYoXrK1ty"
   },
   "source": [
    "# CLIPモデルの仕様確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IBRVTY9lbGm8"
   },
   "outputs": [],
   "source": [
    "# --- CLIPモデルの仕様確認 ----\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "model = torch.jit.load(\"model.pt\").cuda().eval()\n",
    "input_resolution = model.input_resolution.item()\n",
    "context_length = model.context_length.item()\n",
    "vocab_size = model.vocab_size.item()\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmQXf9wrK-6t"
   },
   "source": [
    "# simple_tokenizer の動作確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LFyjzC85LKrH"
   },
   "outputs": [],
   "source": [
    "# テキストをトークンへ変換1\n",
    "index = tokenizer.encode('I ate an apple')\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SgmKMg6mV5RA"
   },
   "outputs": [],
   "source": [
    "# テキストをトークンへ変換2\n",
    "index = tokenizer.encode('image segmentation')\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6H6tX85TKA0n"
   },
   "source": [
    "# 画像の前処理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6cpiIFHp9N6"
   },
   "outputs": [],
   "source": [
    "# --- 画像の前処理 ----\n",
    "\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "# 設定\n",
    "preprocess = Compose([\n",
    "    Resize(input_resolution, interpolation=Image.BICUBIC),\n",
    "    CenterCrop(input_resolution),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "image_mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).cuda()\n",
    "image_std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).cuda()\n",
    "\n",
    "# 前処理実行\n",
    "images =[]\n",
    "files = glob.glob('./food_101/*.jpg')\n",
    "files.sort()\n",
    "for file in files:\n",
    "      image = preprocess(Image.open(file).convert(\"RGB\"))\n",
    "      images.append(image)\n",
    "\n",
    "image_input = torch.tensor(np.stack(images)).cuda()\n",
    "image_input -= image_mean[:, None, None]\n",
    "image_input /= image_std[:, None, None]\n",
    "\n",
    "print('image_input.shape = ', image_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_uKiB2nKQJX"
   },
   "source": [
    "# テキストの前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C4S__zCGy2MT"
   },
   "outputs": [],
   "source": [
    "# --- テキストの前処理 ----\n",
    "\n",
    "# 分類ラベルの設定\n",
    "labels = ['takoyaki', 'susi', 'spagetti', 'ramen', 'pizza', 'omelette', 'humburger', 'gyoza']\n",
    "\n",
    "# ラベルを文の形のトークンへ変換\n",
    "text_descriptions = [f\"This is a photo of a {label}\" for label in labels]  \n",
    "sot_token = tokenizer.encoder['<|startoftext|>']\n",
    "eot_token = tokenizer.encoder['<|endoftext|>']\n",
    "text_tokens = [[sot_token] + tokenizer.encode(desc) + [eot_token] for desc in text_descriptions]\n",
    "text_input = torch.zeros(len(text_tokens), model.context_length, dtype=torch.long)\n",
    "\n",
    "# トークンをテンソルに変換\n",
    "for i, tokens in enumerate(text_tokens):\n",
    "    text_input[i, :len(tokens)] = torch.tensor(tokens)\n",
    "\n",
    "text_input = text_input.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1tFp8PHOKLdE"
   },
   "outputs": [],
   "source": [
    "# 各データの先頭を表示\n",
    "print(text_descriptions[0]) \n",
    "print(text_tokens[0])\n",
    "print(text_input[0])\n",
    "print(text_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2u4oePsAKdJO"
   },
   "source": [
    "# 画像とテキストのcos類似度を計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iqjF6NbBCT0a"
   },
   "outputs": [],
   "source": [
    "# --- 画像とテキストのCOS類似度を計算 ----\n",
    "\n",
    "# CLIPモデルで画像とテキストの特徴を抽出\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image_input).float()\n",
    "    text_features = model.encode_text(text_input).float()\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True) \n",
    "\n",
    "# 画像の特徴とテキストの特徴からCOS類似度を計算\n",
    "text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "top_probs, top_labels = text_probs.cpu().topk(5, dim=-1)\n",
    "\n",
    "print(image_features.shape)\n",
    "print(text_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EYqMIccpabig"
   },
   "outputs": [],
   "source": [
    "# COS類似度の計算結果をそのまま表示\n",
    "print(text_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OIM5PWmSKlVm"
   },
   "source": [
    "# 予測結果の表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s5HrieUc34n_"
   },
   "outputs": [],
   "source": [
    "# --- 予測結果の表示 ---\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def pred_disp(i, image):\n",
    "      plt.figure(figsize=(8, 4))\n",
    "      plt.subplot(1, 2, 1)\n",
    "      plt.imshow(image.permute(1, 2, 0))\n",
    "      plt.axis(\"off\")\n",
    "\n",
    "      plt.subplot(1, 2, 2)\n",
    "      y = np.arange(top_probs.shape[-1])\n",
    "      plt.grid()\n",
    "      plt.barh(y, top_probs[i])\n",
    "      plt.gca().invert_yaxis()\n",
    "      plt.gca().set_axisbelow(True)\n",
    "      plt.yticks(y, [labels[index] for index in top_labels[i].numpy()])\n",
    "      plt.xlabel(\"probability\")\n",
    "\n",
    "      plt.subplots_adjust(wspace=0.5)\n",
    "      plt.show()\n",
    "\n",
    "for i, image in enumerate(images):\n",
    "     pred_disp(i, image)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "CLIP_demo",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
