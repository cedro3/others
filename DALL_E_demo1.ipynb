{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DALL-E_demo1",
      "provenance": [],
      "collapsed_sections": [
        "2nD1n0xEBcko",
        "KAcixx9Z3XYH",
        "JeMCHwDdHIcu",
        "mw0KBLebMywW",
        "UlIUu0jK3S19",
        "GHB7BcR1zNLZ",
        "XaocGDQXz3Zx"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cedro3/others/blob/master/DALL_E_demo1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nD1n0xEBcko"
      },
      "source": [
        "# SetUP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ld7IMAw-EpvF"
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etzxXVZ_r-Nf"
      },
      "source": [
        "!pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 -f https://download.pytorch.org/whl/torch_stable.html ftfy regex\r\n",
        "!pip install DALL-E\r\n",
        "!pip install ftfy\r\n",
        "!git clone https://github.com/openai/CLIP.git\r\n",
        "%cd /content/CLIP/\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAcixx9Z3XYH"
      },
      "source": [
        "# Impot Library & Define\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piJOg9MY7khd"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torchvision.transforms.functional as TF\n",
        "import PIL\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "import imageio\n",
        "from IPython import display\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "import glob\n",
        "from google.colab import output\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "# probably don't mess with this unless you're changing generator size\n",
        "im_shape = [512, 512, 3]\n",
        "sideX, sideY, channels = im_shape\n",
        "\n",
        "def displ(img, pre_scaled=True):\n",
        "  img = np.array(img)[:,:,:]\n",
        "  img = np.transpose(img, (1, 2, 0))\n",
        "  if not pre_scaled:\n",
        "    img = scale(img, 48*4, 32*4)\n",
        "  imageio.imwrite(str(3) + '.png', np.array(img))\n",
        "  return display.Image(str(3)+'.png')\n",
        "\n",
        "def gallery(array, ncols=2):\n",
        "    nindex, height, width, intensity = array.shape\n",
        "    nrows = nindex//ncols\n",
        "    assert nindex == nrows*ncols\n",
        "    # want result.shape = (height*nrows, width*ncols, intensity)\n",
        "    result = (array.reshape(nrows, ncols, height, width, intensity)\n",
        "              .swapaxes(1,2)\n",
        "              .reshape(height*nrows, width*ncols, intensity))\n",
        "    return result\n",
        "\n",
        "def card_padded(im, to_pad=3):\n",
        "  return np.pad(np.pad(np.pad(im, [[1,1], [1,1], [0,0]],constant_values=0), [[2,2], [2,2], [0,0]],constant_values=1),\n",
        "            [[to_pad,to_pad], [to_pad,to_pad], [0,0]],constant_values=0)\n",
        "\n",
        "def get_all(img):\n",
        "  img = np.transpose(img, (0,2,3,1))\n",
        "  cards = np.zeros((img.shape[0], sideX+12, sideY+12, 3))\n",
        "  for i in range(len(img)):\n",
        "    cards[i] = card_padded(img[i])\n",
        "  print(img.shape)\n",
        "  cards = gallery(cards)\n",
        "  imageio.imwrite(str(3) + '.png', np.array(cards))\n",
        "  return display.Image(str(3)+'.png')\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeMCHwDdHIcu"
      },
      "source": [
        "# Perceptor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tm3_VmxpAiB1"
      },
      "source": [
        "import clip\n",
        "clip.available_models()\n",
        "\n",
        "# Load the model\n",
        "perceptor, preprocess = clip.load('ViT-B/32', jit=True)\n",
        "perceptor = perceptor.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHB7BcR1zNLZ"
      },
      "source": [
        "# Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtlDVVMvzMUd"
      },
      "source": [
        "import io\r\n",
        "import os, sys\r\n",
        "import requests\r\n",
        "import PIL\r\n",
        "import torch\r\n",
        "import torchvision.transforms as T\r\n",
        "import torchvision.transforms.functional as TF\r\n",
        "from dall_e import map_pixels, unmap_pixels, load_model\r\n",
        "\r\n",
        "target_image_size = sideX\r\n",
        "\r\n",
        "def preprocess(img):\r\n",
        "    s = min(img.size)\r\n",
        "    \r\n",
        "    if s < target_image_size:\r\n",
        "        raise ValueError(f'min dim for image {s} < {target_image_size}')\r\n",
        "        \r\n",
        "    r = target_image_size / s\r\n",
        "    s = (round(r * img.size[1]), round(r * img.size[0]))\r\n",
        "    img = TF.resize(img, s, interpolation=PIL.Image.LANCZOS)\r\n",
        "    img = TF.center_crop(img, output_size=2 * [target_image_size])\r\n",
        "    img = torch.unsqueeze(T.ToTensor()(img), 0)\r\n",
        "    return map_pixels(img)\r\n",
        "\r\n",
        "model = load_model(\"https://cdn.openai.com/dall-e/decoder.pkl\", 'cuda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaocGDQXz3Zx"
      },
      "source": [
        "# Text_input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AI9infK-1VEG"
      },
      "source": [
        "text_input = \"Many people are flying with umbrellas in New York\" \n",
        "tau_value =1.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TazvQEzvzzdS"
      },
      "source": [
        "# Latent coordinate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdCh2D8Dt8Xd"
      },
      "source": [
        "class Pars(torch.nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(Pars, self).__init__()\r\n",
        "        self.normu = torch.nn.Parameter(torch.randn(1, 8192, 64, 64).cuda())\r\n",
        "\r\n",
        "    def forward(self):\r\n",
        "      # normu = torch.nn.functional.gumbel_softmax(self.normu.view(1, 8192, -1), dim=-1).view(1, 8192, 64, 64)\r\n",
        "      normu = torch.nn.functional.gumbel_softmax(self.normu.view(1, 8192, -1), dim=-1, tau=tau_value).view(1, 8192, 64, 64)\r\n",
        "      return normu\r\n",
        "\r\n",
        "lats = Pars().cuda()\r\n",
        "mapper = [lats.normu]\r\n",
        "optimizer = torch.optim.Adam([{'params': mapper, 'lr': .1}])\r\n",
        "eps = 0\r\n",
        "tx = clip.tokenize(text_input)\r\n",
        "t = perceptor.encode_text(tx.cuda()).detach().clone()\r\n",
        "nom = torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\r\n",
        "\r\n",
        "with torch.no_grad():\r\n",
        "  mult = 1\r\n",
        "  al = unmap_pixels(torch.sigmoid(model(lats()).cpu().float())).numpy()\r\n",
        "  for allls in al:\r\n",
        "    displ(allls[:3])\r\n",
        "    print('\\n')\r\n",
        "  # print(torch.topk(lats().view(1, 8192, -1), k=3, dim=-1))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WztSrRF23Rqg"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EuUz-ICNKUr"
      },
      "source": [
        "def checkin(loss):\n",
        "  print('''########################################################## ''',loss, '\\n',itt)\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    al = unmap_pixels(torch.sigmoid(model(lats())[:, :3]).cpu().float()).numpy()\n",
        "  for allls in al:\n",
        "    displ(allls)\n",
        "    display.display(display.Image(str(3)+'.png'))\n",
        "    print('\\n')\n",
        "  # the people spoke and they love \"ding\"\n",
        "  output.eval_js('new Audio(\"https://freesound.org/data/previews/80/80921_1022651-lq.ogg\").play()')\n",
        "\n",
        "def ascend_txt():\n",
        "  out = unmap_pixels(torch.sigmoid(model(lats())[:, :3].float()))\n",
        "  cutn = 64 # improves quality\n",
        "  p_s = []\n",
        "  for ch in range(cutn):\n",
        "    size = int(sideX*torch.zeros(1,).normal_(mean=.8, std=.3).clip(.5, .98))\n",
        "    offsetx = torch.randint(0, sideX - size, ())\n",
        "    offsety = torch.randint(0, sideX - size, ())\n",
        "    apper = out[:, :, offsetx:offsetx + size, offsety:offsety + size]\n",
        "    apper = torch.nn.functional.interpolate(apper, (224,224), mode='bilinear')\n",
        "    p_s.append(apper)\n",
        "  into = torch.cat(p_s, 0)\n",
        "  # into = torch.nn.functional.interpolate(out, (224,224), mode='nearest')\n",
        "  into = nom(into)\n",
        "  iii = perceptor.encode_image(into)\n",
        "  llls = lats()\n",
        "  lat_l = 0\n",
        "  return [lat_l, 10*-torch.cosine_similarity(t, iii).view(-1, 1).T.mean(1)]\n",
        "\n",
        "def train(i):\n",
        "  loss1 = ascend_txt()\n",
        "  loss = loss1[0] + loss1[1]\n",
        "  loss = loss.mean()\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "    \n",
        "  if itt % 100 == 0:\n",
        "    checkin(loss1)\n",
        "    shutil.copy('./3.png', './images/%s.png'%str(int(itt/100)).zfill(6))\n",
        "\n",
        "import shutil\n",
        "\n",
        "if os.path.isdir('images'):\n",
        "    shutil.rmtree('images')\n",
        "os.makedirs('images', exist_ok=True)\n",
        "\n",
        "itt = 0\n",
        "for asatreat in range(1100):\n",
        "  train(itt)\n",
        "  itt+=1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnPJLK5gg0BO"
      },
      "source": [
        "# Make movie"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoD69Lkrtjdy"
      },
      "source": [
        "if os.path.exists('./output.mp4'):\n",
        "   os.remove('./output.mp4')\n",
        "\n",
        "!ffmpeg -r 2 -i images/%06d.png -vcodec libx264 -pix_fmt yuv420p output.mp4"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}