{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SwapAE",
      "provenance": [],
      "authorship_tag": "ABX9TyPjOM3j/DkMSCkMdDsEhQV7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cedro3/others/blob/master/SwapAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fEyCmb3Drl2"
      },
      "outputs": [],
      "source": [
        "# githubからコードをコピー\n",
        "! git clone https://github.com/bryandlee/naver-webtoon-faces.git\n",
        "%cd naver-webtoon-faces\n",
        " \n",
        "# 学習済みパラメータのダウンロード\n",
        "! pip install --upgrade gdown\n",
        "import gdown\n",
        "gdown.download('https://drive.google.com/uc?id=1gJ5WPFQIN26xYbujrEAKxG7YduE9S6ch', './checkpoint.zip', quiet=False)\n",
        "! unzip checkpoint.zip\n",
        " \n",
        "# resultsフォルダーを作成\n",
        "import os\n",
        "os.makedirs('results', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 関数定義\n",
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        " \n",
        "def load_image(path, size):\n",
        "    image = image2tensor(cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB))\n",
        " \n",
        "    w, h = image.shape[-2:]\n",
        "    if w != h:\n",
        "        crop_size = min(w, h)\n",
        "        left = (w - crop_size)//2\n",
        "        right = left + crop_size\n",
        "        top = (h - crop_size)//2\n",
        "        bottom = top + crop_size\n",
        "        image = image[:,:,left:right, top:bottom]\n",
        " \n",
        "    if image.shape[-1] != size:\n",
        "        image = torch.nn.functional.interpolate(image, (size, size), mode=\"bilinear\", align_corners=True)\n",
        "    \n",
        "    return image\n",
        " \n",
        "def image2tensor(image):\n",
        "    image = torch.FloatTensor(image).permute(2,0,1).unsqueeze(0)/255.\n",
        "    return (image-0.5)/0.5\n",
        " \n",
        "def tensor2image(tensor):\n",
        "    tensor = tensor.clamp(-1., 1.).detach().squeeze().permute(1,2,0).cpu().numpy()\n",
        "    return tensor*0.5 + 0.5\n",
        " \n",
        "def imshow(img, size=5, cmap='jet'):\n",
        "    plt.figure(figsize=(size,size))\n",
        "    plt.imshow(img, cmap=cmap)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        " \n",
        "def horizontal_concat(imgs):\n",
        "    return torch.cat([img.unsqueeze(0) for img in imgs], 3) \n",
        " \n",
        "device = 'cuda:0'\n",
        "image_size = 256\n",
        "torch.set_grad_enabled(False)"
      ],
      "metadata": {
        "id": "9Unj3DF1DzBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SwapAEモデルのロード\n",
        "from model import Encoder, Generator\n",
        " \n",
        "ae_model_path = './checkpoint/002000.pt'\n",
        "        \n",
        "encoder = Encoder(32).to(device)\n",
        "generator = Generator(32).to(device)\n",
        " \n",
        "ckpt = torch.load(ae_model_path, map_location=device)\n",
        "encoder.load_state_dict(ckpt[\"e_ema\"])\n",
        "generator.load_state_dict(ckpt[\"g_ema\"])\n",
        " \n",
        "encoder.eval()\n",
        "generator.eval()\n",
        " \n",
        "print(f'[SwapAE model loaded] {ae_model_path}')"
      ],
      "metadata": {
        "id": "GK-YnIxcD6ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stylegan2.model import Generator as StyleGAN\n",
        " \n",
        "stylegan_model_path = './checkpoint/stylegan2-naverwebtoon-800k.pt'\n",
        "stylegan_ckpt = torch.load(stylegan_model_path, map_location=device)\n",
        " \n",
        "latent_dim = stylegan_ckpt['args'].latent\n",
        " \n",
        "stylegan = StyleGAN(image_size, latent_dim, 8).to(device)\n",
        "stylegan.load_state_dict(stylegan_ckpt[\"g_ema\"], strict=False)\n",
        "stylegan.eval()\n",
        "print(f'[StyleGAN2 generator loaded] {stylegan_model_path}\\n')\n",
        " \n",
        "truncation = 0.7\n",
        "trunc = stylegan.mean_latent(4096).detach().clone()\n",
        " \n",
        "num_samples = 8\n",
        " \n",
        "latent = stylegan.get_latent(torch.randn(num_samples, latent_dim, device=device))\n",
        "imgs_gen, _ = stylegan([latent],\n",
        "                        truncation=truncation,\n",
        "                        truncation_latent=trunc,\n",
        "                        input_is_latent=True,\n",
        "                        randomize_noise=True)\n",
        " \n",
        "print(\"StyleGAN2 generated images:\")\n",
        "imshow(tensor2image(horizontal_concat(imgs_gen)), size=20)\n",
        " \n",
        "structures, textures = encoder(imgs_gen)\n",
        "recon_results = generator(structures, textures)\n",
        " \n",
        "print(\"SwapAE reconstructions:\")    \n",
        "imshow(tensor2image(horizontal_concat(recon_results)), size=20)\n",
        " \n",
        "print(\"Swapping results:\")    \n",
        "swap_results = generator(structures, textures[0].unsqueeze(0).repeat(num_samples,1))\n",
        "imshow(tensor2image(horizontal_concat(swap_results)), size=20)"
      ],
      "metadata": {
        "id": "N5ZQWGNpD-2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_image_path = \"./inputs/6.jpg\"\n",
        "test_image = load_image(test_image_path, image_size)\n",
        " \n",
        "num_styles = 5\n",
        " \n",
        "latent = stylegan.get_latent(torch.randn(num_styles, latent_dim, device=device))\n",
        "imgs_gen, _ = stylegan([latent],\n",
        "                        truncation=truncation,\n",
        "                        truncation_latent=trunc,\n",
        "                        input_is_latent=True,\n",
        "                        randomize_noise=True)\n",
        " \n",
        "inputs = torch.cat([test_image.to(device), imgs_gen])\n",
        " \n",
        "results = horizontal_concat(inputs.cpu())\n",
        " \n",
        "structures, target_textures = encoder(inputs)\n",
        " \n",
        "structure = structures[0].unsqueeze(0).repeat(len(target_textures),1,1,1)\n",
        "source_texture = target_textures[0].unsqueeze(0).repeat(len(target_textures),1)\n",
        " \n",
        "for swap_loc in [1, 5]:\n",
        "    textures = [source_texture for _ in range(swap_loc)] + [target_textures for _ in range(len(generator.layers) - swap_loc)]        \n",
        "    fake_imgs = generator(structure, textures, noises=0)\n",
        " \n",
        "    results = torch.cat([results, horizontal_concat(fake_imgs).cpu()], dim=2)\n",
        "        \n",
        "imshow(tensor2image(results), 23)\n",
        " \n",
        "cv2.imwrite('./results/out.jpg', cv2.cvtColor(255*tensor2image(results), cv2.COLOR_BGR2RGB))"
      ],
      "metadata": {
        "id": "1L8XDxnSEG5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "        \n",
        "swap_loc = 1\n",
        " \n",
        "num_anchors = 10\n",
        "num_interp = 20\n",
        "anchors = stylegan.get_latent(torch.randn(num_anchors, 512, device=device))\n",
        " \n",
        "photo_input = test_image.to(device)\n",
        "ori_structure, ori_textures = encoder(photo_input)\n",
        "    \n",
        "black_image = torch.zeros_like(test_image)\n",
        " \n",
        "with imageio.get_writer('results/exploration.gif', mode='I', duration=0.05, palettesize=256, subrectangles=False) as writer:\n",
        "    \n",
        "    for i in tqdm(range(num_anchors-1)):\n",
        "        initial = anchors[i]\n",
        "        final = anchors[i+1]\n",
        " \n",
        "        for j in range(num_interp):\n",
        "            latent = (float(num_interp-j) * initial + float(j) * final)/num_interp\n",
        " \n",
        "            gen_img, _ = stylegan([latent],\n",
        "                                   truncation=truncation,\n",
        "                                   truncation_latent=trunc,\n",
        "                                   input_is_latent=True,\n",
        "                                   randomize_noise=True)\n",
        " \n",
        "            _, target_texture = encoder(gen_img)\n",
        "            textures = [ori_textures for _ in range(swap_loc)] + [target_texture for _ in range(len(generator.layers) - swap_loc)]\n",
        "            swap_img = generator(ori_structure, textures, noises=0)\n",
        " \n",
        "            result = torch.cat([black_image, gen_img.cpu()], 3)\n",
        "            result = torch.cat([\n",
        "                result,\n",
        "                torch.cat([test_image, swap_img.cpu()], 3)\n",
        "            ], 2)\n",
        "            \n",
        "            writer.append_data((tensor2image(result)*255).astype(np.uint8))\n",
        " \n",
        "# output.mp4をリセット\n",
        "if os.path.exists('./output.mp4'):\n",
        "   os.remove('./output.mp4')\n",
        " \n",
        "# GIFからmp4を作成\n",
        "! ffmpeg -i results/exploration.gif  -movflags faststart -pix_fmt yuv420p -vf \"scale=trunc(iw/2)*2:trunc(ih/2)*2\" output.mp4"
      ],
      "metadata": {
        "id": "iWz354d1ERBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mp4動画の再生\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        " \n",
        "mp4 = open('./output.mp4', 'rb').read()\n",
        "data_url = 'data:video/mp4;base64,' + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"\n",
        "<video width=\"50%\" height=\"50%\" controls>\n",
        "      <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "</video>\"\"\")"
      ],
      "metadata": {
        "id": "MCD2bnp_EkCb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}