{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PIFuHD_movie",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cedro3/others/blob/master/PIFuHD_movie.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYhlsDkg1Hwb"
      },
      "source": [
        "## セットアップ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8B1jmr82DtjG"
      },
      "source": [
        "# ライブラリー取得\n",
        "!pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install pytorch3d\n",
        "\n",
        "# githubからpifuhdのコードをコピー\n",
        "!git clone https://github.com/facebookresearch/pifuhd\n",
        "\n",
        "# githubからpose-estimationのコードをコピーし、学習済み重みをダウンロード\n",
        "!git clone https://github.com/Daniil-Osokin/lightweight-human-pose-estimation.pytorch.git\n",
        "%cd /content/lightweight-human-pose-estimation.pytorch/\n",
        "!wget https://download.01.org/opencv/openvino_training_extensions/models/human_pose_estimation/checkpoint_iter_370000.pth\n",
        "\n",
        "# pifuhdの学習済み重みをダンロード\n",
        "%cd /content/pifuhd/\n",
        "!sh ./scripts/download_trained_model.sh\n",
        "\n",
        "# サンプルビデオダウンロード\n",
        "import gdown\n",
        "gdown.download('https://drive.google.com/uc?id=1rrccXA-k-45cUx1MDoLH6O3M4RF5pA_E', 'movie.zip', quiet=False)\n",
        "! unzip movie.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvQm-A8ESKb2"
      },
      "source": [
        "## サンプルビデオから静止画を切り出す\n",
        "video_file = : ビデオ指定\\\n",
        "interval = ：　静止画の切り出し間隔"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0iVZpOdwC3d"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import cv2\n",
        " \n",
        "# imagesフォルダーリセット\n",
        "if os.path.isdir('images'):\n",
        "    shutil.rmtree('images') \n",
        "os.makedirs('images', exist_ok=True)\n",
        " \n",
        "def video_2_images(video_file= './movie/01.mp4', \n",
        "                   image_dir='./images/', \n",
        "                   image_file='%s.png'):\n",
        " \n",
        "    # Initial setting\n",
        "    i = 0\n",
        "    interval = 3\n",
        "    length = 300 # リミッター\n",
        "    \n",
        "    cap = cv2.VideoCapture(video_file)\n",
        "    while(cap.isOpened()):\n",
        "        flag, frame = cap.read()  \n",
        "        if flag == False:  \n",
        "                break\n",
        "        if i == length*interval:\n",
        "                break\n",
        "        if i % interval == 0:    \n",
        "           cv2.imwrite(image_dir+image_file % str(int(i/interval)).zfill(6), frame)\n",
        "        i += 1 \n",
        "    cap.release()  \n",
        " \n",
        "video_2_images()\n",
        "list_d = os.listdir('./images')\n",
        "num = len(list_d)\n",
        "print(num)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0x-pUOPT6bIM"
      },
      "source": [
        "## クロップ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdRcDXe38lHB"
      },
      "source": [
        "%cd /content/lightweight-human-pose-estimation.pytorch/\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from models.with_mobilenet import PoseEstimationWithMobileNet\n",
        "from modules.keypoints import extract_keypoints, group_keypoints\n",
        "from modules.load_state import load_state\n",
        "from modules.pose import Pose, track_poses\n",
        "import demo\n",
        "\n",
        "def get_rect(net, images, height_size):\n",
        "    net = net.eval()\n",
        "\n",
        "    stride = 8\n",
        "    upsample_ratio = 4\n",
        "    num_keypoints = Pose.num_kpts\n",
        "    previous_poses = []\n",
        "    delay = 33\n",
        "    for image in images:\n",
        "        rect_path = image.replace('.%s' % (image.split('.')[-1]), '_rect.txt')\n",
        "        img = cv2.imread(image, cv2.IMREAD_COLOR)\n",
        "        orig_img = img.copy()\n",
        "        orig_img = img.copy()\n",
        "        heatmaps, pafs, scale, pad = demo.infer_fast(net, img, height_size, stride, upsample_ratio, cpu=False)\n",
        "\n",
        "        total_keypoints_num = 0\n",
        "        all_keypoints_by_type = []\n",
        "        for kpt_idx in range(num_keypoints):  # 19th for bg\n",
        "            total_keypoints_num += extract_keypoints(heatmaps[:, :, kpt_idx], all_keypoints_by_type, total_keypoints_num)\n",
        "\n",
        "        pose_entries, all_keypoints = group_keypoints(all_keypoints_by_type, pafs) #, demo=True)\n",
        "        for kpt_id in range(all_keypoints.shape[0]):\n",
        "            all_keypoints[kpt_id, 0] = (all_keypoints[kpt_id, 0] * stride / upsample_ratio - pad[1]) / scale\n",
        "            all_keypoints[kpt_id, 1] = (all_keypoints[kpt_id, 1] * stride / upsample_ratio - pad[0]) / scale\n",
        "        current_poses = []\n",
        "\n",
        "        rects = []\n",
        "        for n in range(len(pose_entries)):\n",
        "            if len(pose_entries[n]) == 0:\n",
        "                continue\n",
        "            pose_keypoints = np.ones((num_keypoints, 2), dtype=np.int32) * -1\n",
        "            valid_keypoints = []\n",
        "            for kpt_id in range(num_keypoints):\n",
        "                if pose_entries[n][kpt_id] != -1.0:  # keypoint was found\n",
        "                    pose_keypoints[kpt_id, 0] = int(all_keypoints[int(pose_entries[n][kpt_id]), 0])\n",
        "                    pose_keypoints[kpt_id, 1] = int(all_keypoints[int(pose_entries[n][kpt_id]), 1])\n",
        "                    valid_keypoints.append([pose_keypoints[kpt_id, 0], pose_keypoints[kpt_id, 1]])\n",
        "            valid_keypoints = np.array(valid_keypoints)\n",
        "            \n",
        "            if pose_entries[n][10] != -1.0 or pose_entries[n][13] != -1.0:\n",
        "              pmin = valid_keypoints.min(0)\n",
        "              pmax = valid_keypoints.max(0)\n",
        "\n",
        "              center = (0.5 * (pmax[:2] + pmin[:2])).astype(np.int)\n",
        "              radius = int(0.65 * max(pmax[0]-pmin[0], pmax[1]-pmin[1]))\n",
        "            elif pose_entries[n][10] == -1.0 and pose_entries[n][13] == -1.0 and pose_entries[n][8] != -1.0 and pose_entries[n][11] != -1.0:\n",
        "              # if leg is missing, use pelvis to get cropping\n",
        "              center = (0.5 * (pose_keypoints[8] + pose_keypoints[11])).astype(np.int)\n",
        "              radius = int(1.45*np.sqrt(((center[None,:] - valid_keypoints)**2).sum(1)).max(0))\n",
        "              center[1] += int(0.05*radius)\n",
        "            else:\n",
        "              center = np.array([img.shape[1]//2,img.shape[0]//2])\n",
        "              radius = max(img.shape[1]//2,img.shape[0]//2)\n",
        "\n",
        "            x1 = center[0] - radius\n",
        "            y1 = center[1] - radius\n",
        "\n",
        "            rects.append([x1, y1, 2*radius, 2*radius])\n",
        "\n",
        "        np.savetxt(rect_path, np.array(rects), fmt='%d')\n",
        "\n",
        "net = PoseEstimationWithMobileNet()\n",
        "checkpoint = torch.load('checkpoint_iter_370000.pth', map_location='cpu')\n",
        "load_state(net, checkpoint)\n",
        "\n",
        "# get_rect ループ\n",
        "from tqdm import trange\n",
        "for i in trange(num):\n",
        "    image_path = '/content/pifuhd/images/'+str(i).zfill(6)+'.png'\n",
        "    get_rect(net.cuda(), [image_path], 512)\n",
        "\n",
        "# 全ての rect.txt を同じ値に揃える\n",
        "import shutil\n",
        "for i in range(1,num):\n",
        "    shutil.copyfile('/content/pifuhd/images/000000_rect.txt', '/content/pifuhd/images/'+str(i).zfill(6)+'_rect.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlWPwwuj6fTb"
      },
      "source": [
        "## objファイルの作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40UE4Cp37HV1"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# results/pifuhd_final/reconフォルダーリセット\n",
        "if os.path.isdir('results/pifuhd_final/recon'):\n",
        "    shutil.rmtree('results/pifuhd_final/recon') \n",
        "os.makedirs('results/pifuhd_final/recon', exist_ok=True)\n",
        "\n",
        "# objファイル作成\n",
        "%cd /content/pifuhd/\n",
        "!python -m apps.simple_test -r 256 --use_rect -i './images'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFR6HYh36sEU"
      },
      "source": [
        "## objファイルから３Dモデル画像生成"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bELoZ47I1NW-"
      },
      "source": [
        "**関数定義**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcEs-e3zX2xd"
      },
      "source": [
        "import io\n",
        "import os\n",
        "import torch\n",
        "from skimage.io import imread\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "# Util function for loading meshes\n",
        "from pytorch3d.io import load_objs_as_meshes\n",
        "\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "# Data structures and functions for rendering\n",
        "from pytorch3d.structures import Meshes\n",
        "from pytorch3d.renderer import (\n",
        "    look_at_view_transform,\n",
        "    OpenGLOrthographicCameras, \n",
        "    PointLights, \n",
        "    DirectionalLights, \n",
        "    Materials, \n",
        "    RasterizationSettings, \n",
        "    MeshRenderer, \n",
        "    MeshRasterizer,  \n",
        "    HardPhongShader,\n",
        "    TexturesVertex\n",
        ")\n",
        "\n",
        "def set_renderer():\n",
        "    # Setup\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    torch.cuda.set_device(device)\n",
        "\n",
        "    # Initialize an OpenGL perspective camera.\n",
        "    R, T = look_at_view_transform(2.0, 0, 180) \n",
        "    cameras = OpenGLOrthographicCameras(device=device, R=R, T=T)\n",
        "\n",
        "    raster_settings = RasterizationSettings(\n",
        "        image_size=512, \n",
        "        blur_radius=0.0, \n",
        "        faces_per_pixel=1, \n",
        "        bin_size = None, \n",
        "        max_faces_per_bin = None\n",
        "    )\n",
        "\n",
        "    lights = PointLights(device=device, location=((2.0, 2.0, 2.0),))\n",
        "\n",
        "    renderer = MeshRenderer(\n",
        "        rasterizer=MeshRasterizer(\n",
        "            cameras=cameras, \n",
        "            raster_settings=raster_settings\n",
        "        ),\n",
        "        shader=HardPhongShader(\n",
        "            device=device, \n",
        "            cameras=cameras,\n",
        "            lights=lights\n",
        "        )\n",
        "    )\n",
        "    return renderer\n",
        "\n",
        "def get_verts_rgb_colors(obj_path):\n",
        "  rgb_colors = []\n",
        "\n",
        "  f = open(obj_path)\n",
        "  lines = f.readlines()\n",
        "  for line in lines:\n",
        "    ls = line.split(' ')\n",
        "    if len(ls) == 7:\n",
        "      rgb_colors.append(ls[-3:])\n",
        "\n",
        "  return np.array(rgb_colors, dtype='float32')[None, :, :]\n",
        "\n",
        "def generate_image_from_obj(obj_path, image_path, renderer):  \n",
        "    input_image = cv2.imread(image_path)\n",
        "    input_image = input_image[:,:input_image.shape[1]//3]\n",
        "    input_image = cv2.resize(input_image, (512,512))\n",
        "\n",
        "    # Setup\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    torch.cuda.set_device(device)\n",
        "\n",
        "    # Load obj file\n",
        "    verts_rgb_colors = get_verts_rgb_colors(obj_path)\n",
        "    verts_rgb_colors = torch.from_numpy(verts_rgb_colors).to(device)\n",
        "    textures = TexturesVertex(verts_features=verts_rgb_colors)\n",
        "\n",
        "    # Load obj\n",
        "    mesh = load_objs_as_meshes([obj_path], device=device)\n",
        "\n",
        "    # Set mesh\n",
        "    vers = mesh._verts_list\n",
        "    faces = mesh._faces_list\n",
        "    mesh_w_tex = Meshes(vers, faces, textures)\n",
        "\n",
        "    R, T = look_at_view_transform(1.8, 0, 0, device=device)\n",
        "    images_w_tex = renderer(mesh_w_tex, R=R, T=T)\n",
        "    images_w_tex = np.clip(images_w_tex[0, ..., :3].cpu().numpy(), 0.0, 1.0)[:, :, ::-1] * 255\n",
        "    cv2.imwrite('./out1/'+filename, images_w_tex) \n",
        "\n",
        "    image = np.concatenate([input_image, images_w_tex], axis=1)\n",
        "    cv2.imwrite('./out2/'+filename, image)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uS8khy7e1T4c"
      },
      "source": [
        "**３Dモデル画像生成**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-ajEW-eepOl"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "from tqdm import trange\n",
        "\n",
        "if os.path.isdir('out1'):\n",
        "     shutil.rmtree('out1')\n",
        "os.makedirs('out1', exist_ok=True)\n",
        "\n",
        "if os.path.isdir('out2'):\n",
        "     shutil.rmtree('out2')\n",
        "os.makedirs('out2', exist_ok=True)\n",
        "\n",
        "for i in trange(num):\n",
        "    filename = str(i).zfill(6)+'.png'\n",
        "    out_img_path = '/content/pifuhd/results/pifuhd_final/recon/result_'+str(i).zfill(6)+'_256.png'\n",
        "    obj_path = '/content/pifuhd/results/pifuhd_final/recon/result_'+str(i).zfill(6)+'_256.obj'\n",
        "    renderer = set_renderer()\n",
        "    generate_image_from_obj(obj_path, out_img_path, renderer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilbSPGC86x86"
      },
      "source": [
        "## 画像から動画の生成"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nHKzjrG4B_N"
      },
      "source": [
        "**３Dモデル**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqjTPzNf8fnN"
      },
      "source": [
        "# 既に output1.mp4 があれば削除\n",
        "import os\n",
        "if os.path.exists('./output1.mp4'):\n",
        "   os.remove('./output1.mp4')\n",
        "\n",
        "! ffmpeg -r 10 -i out1/%6d.png\\\n",
        "               -vcodec libx264 -pix_fmt yuv420p output1.mp4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEF9wEih3bCz"
      },
      "source": [
        "# --- mp4動画の再生 ---\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "mp4 = open('./output1.mp4', 'rb').read()\n",
        "data_url = 'data:video/mp4;base64,' + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"\n",
        "<video width=\"40%\" height=\"40%\" controls>\n",
        "      <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "</video>\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUDyy7Bs4UE0"
      },
      "source": [
        "**ビデオ＋3Dモデル**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtEeBO8N20w7"
      },
      "source": [
        "# 既に output2.mp4 があれば削除\n",
        "import os\n",
        "if os.path.exists('./output2.mp4'):\n",
        "   os.remove('./output2.mp4')\n",
        "\n",
        "! ffmpeg -r 10 -i out2/%6d.png\\\n",
        "               -vcodec libx264 -pix_fmt yuv420p output2.mp4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzvADdr43dt4"
      },
      "source": [
        "# --- mp4動画の再生 ---\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "mp4 = open('./output2.mp4', 'rb').read()\n",
        "data_url = 'data:video/mp4;base64,' + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"\n",
        "<video width=\"70%\" height=\"70%\" controls>\n",
        "      <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "</video>\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}